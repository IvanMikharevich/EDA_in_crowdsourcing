{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "yandex_task_2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "DgnGlaDbqHaI",
        "outputId": "e1811a30-9262-4f7a-c058-e561f960714b"
      },
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-98d6d155-098d-485c-b884-6eb31ef2da92\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-98d6d155-098d-485c-b884-6eb31ef2da92\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving файл 2.csv to файл 2.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HxCGXT2fqOCJ"
      },
      "source": [
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "import os\n",
        "from sklearn.metrics import classification_report, fbeta_score, accuracy_score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gv6KtXHJqVos"
      },
      "source": [
        "Загрузим файл, переведём его в dataframe и посмотрим на основную информацию о столбцах, также посмортим на саму таблицу"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lUZKSFkaqQhw",
        "outputId": "8646568c-eb3e-4b59-b095-7c353f62694a"
      },
      "source": [
        "def get_df(dir):\n",
        "    df = pd.read_csv(dir, sep='\\t', header=0, skipinitialspace=True)\n",
        "    return df\n",
        "\n",
        "workdir = Path(os.getcwd())\n",
        "df = get_df(workdir/'файл 2.csv')\n",
        "print(df.info())\n",
        "print(df.head(5))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 250000 entries, 0 to 249999\n",
            "Data columns (total 5 columns):\n",
            " #   Column  Non-Null Count   Dtype \n",
            "---  ------  --------------   ----- \n",
            " 0   login   250000 non-null  object\n",
            " 1   uid     250000 non-null  int64 \n",
            " 2   docid   250000 non-null  int64 \n",
            " 3   jud     250000 non-null  int64 \n",
            " 4   cjud    250000 non-null  int64 \n",
            "dtypes: int64(4), object(1)\n",
            "memory usage: 9.5+ MB\n",
            "None\n",
            "         login  uid  docid  jud  cjud\n",
            "0  assessor158  158      0    0     0\n",
            "1  assessor238  238      0    0     0\n",
            "2  assessor488  488      0    0     0\n",
            "3  assessor136  136      0    0     0\n",
            "4  assessor300  300      0    0     0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6pCUqtQcQBTf"
      },
      "source": [
        "Посмотрим на количество оценок каждого асессора. Минимальное количество оценок у ассесора равно 99, значит можно спокойно оценивать работу каждого ассесора "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J4KF5nUxueB-",
        "outputId": "ec47b2f6-644e-4f5e-d37c-ba0c037698d7"
      },
      "source": [
        "print(df['login'].value_counts())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "assessor191    484\n",
            "assessor140    481\n",
            "assessor161    480\n",
            "assessor236    467\n",
            "assessor165    467\n",
            "              ... \n",
            "assessor420    365\n",
            "assessor202    365\n",
            "assessor255    364\n",
            "assessor550    128\n",
            "assessor234     99\n",
            "Name: login, Length: 600, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "71eCJ6JPP5LW"
      },
      "source": [
        "Для оценки работы ассесоров можно применить стандартые метрики классификации: accuracy, precision, recall, f1-score. Для этого напишем функцию агрегации по датафрефрейму сгруппированому по user_id. По итогу получим dataframe со всеми перечисленными метриками для каждого ассесора."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PtEtfLBbujCl",
        "outputId": "d3db172c-7871-49b6-899e-624b584fde59"
      },
      "source": [
        "group_login = df.groupby('uid', as_index=False)\n",
        "\n",
        "def get_metriс(df):\n",
        "  clf_report = classification_report(df['cjud'], df['jud'], output_dict=True)\n",
        "  result = clf_report['1']\n",
        "  result['accuracy'] = clf_report['accuracy']\n",
        "  del result['support']\n",
        "  return pd.Series(result, index=['accuracy', 'precision', 'recall', 'f1-score'])\n",
        "\n",
        "metric_df = group_login.apply(get_metriс)\n",
        "print(metric_df)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     uid  accuracy  precision    recall  f1-score\n",
            "0      0  0.837905   0.453704  0.890909  0.601227\n",
            "1      1  0.800971   0.344538  0.911111  0.500000\n",
            "2      2  0.799472   0.403670  0.800000  0.536585\n",
            "3      3  0.460094   0.108225  0.510204  0.178571\n",
            "4      4  0.827751   0.452991  0.868852  0.595506\n",
            "..   ...       ...        ...       ...       ...\n",
            "595  595  0.849099   0.393617  0.787234  0.524823\n",
            "596  596  0.917085   0.610390  0.940000  0.740157\n",
            "597  597  0.892774   0.581081  0.741379  0.651515\n",
            "598  598  0.775120   0.330645  0.788462  0.465909\n",
            "599  599  0.905660   0.605263  0.821429  0.696970\n",
            "\n",
            "[600 rows x 5 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q5rBrOGaTQGN"
      },
      "source": [
        "Немного о природе этого бинарного задания. Ассесор будет присваивать 1, если документ не релевантен."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gRBS-qcvRZiS"
      },
      "source": [
        "accuracy - это процент верно распознаных классов 1 или 0. Это простая метрика может стать основной для оценки работы ассесоров. Только 25% ассесоров верно распознают класс с вероятностью ниже 0.81. Другие 25% ассеосоров верно распознают класс с вероятностью выше 0.89 (при описании цифры немного округляю). Средняя вероятность правильного распознования класса у ассесора равна 0.84."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xD4mMCWh_scq",
        "outputId": "769ad663-80ab-4d18-8bef-2cef4a7eadf3"
      },
      "source": [
        "print(metric_df['accuracy'].describe())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "count    600.000000\n",
            "mean       0.840658\n",
            "std        0.055462\n",
            "min        0.425791\n",
            "25%        0.808538\n",
            "50%        0.833333\n",
            "75%        0.887500\n",
            "max        0.923810\n",
            "Name: accuracy, dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k1owulkgUnK1"
      },
      "source": [
        "Precision можно интерпретировать как долю документов, отнесённых ассесором к классу 1 и при этом действительно являющимися релевантными. Это означает, что нам не так важно найти все нерелевантные документы, как быть уверенным в том, что среди документов названых ассесором нерелевантными большинство окажется нерелевантными. Эта метрика нам точно не подходит, поскольку она не контролирует количество нерелевантных документов ненайденных ассесором. Если ассесор узнает, что его работу оценивают такой метрикой, то он может начать отмечать как нерелевантные документы только те, в которых он хорошо уверен. И если ассесор увидит документ, с которым 'не все так просто', он может просто пропусть его и на оценку это не повлияет."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eouji2nuUnnS",
        "outputId": "3524b156-4b4c-4f20-d928-a6d43eb43a93"
      },
      "source": [
        "print(metric_df['precision'].describe())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "count    600.000000\n",
            "mean       0.431491\n",
            "std        0.101006\n",
            "min        0.097458\n",
            "25%        0.361286\n",
            "50%        0.411528\n",
            "75%        0.515396\n",
            "max        0.666667\n",
            "Name: precision, dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eTTso8LjXBGA"
      },
      "source": [
        "Recall показывает какую долю документов ассесор отметил как нерелевантные из всех реально нерелевантных документов. Эта метрика чуть больше подходит для этой задачи, но все равно плоха. Поскольку нам важно распознать как можно больше нерелеватных документов, но ассесор может начать бездумно отмечать документы как нерелевантные. Это будет неприятно для многих релевантных документов. Собственно говоря, если сравнить precision и recall, то видно, что если ассесор сомневается, то он скорее отнесет документ к нерелеватному. Об этом говорит, то что precision заметно меньше, чем recall."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7FjnJ-FPXB0Z",
        "outputId": "d38d42bf-444d-4167-b10b-888ae17de9b4"
      },
      "source": [
        "print(metric_df['recall'].describe())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "count    600.000000\n",
            "mean       0.830376\n",
            "std        0.086267\n",
            "min        0.105263\n",
            "25%        0.796296\n",
            "50%        0.845491\n",
            "75%        0.886364\n",
            "max        0.982456\n",
            "Name: recall, dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NRZBnmDsZZhJ"
      },
      "source": [
        "F-мера это среднее гармоническое (с множителем 2, чтобы в случае precision = 1 и recall = 1 иметь F = 1). F-мера достигает максимума при recall и precision, равными единице, и близка к нулю, если один из аргументов близок к нулю. Эта метрика хорошо подходит для данной задачи. Нам становиться важно, чтобы ассесор с одной стороны нашел как можно больше нерелеватных документов, при этом старался походу не 'зацеплять' релеватные документы."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P86p0Cm5ZZ45",
        "outputId": "8ca396f7-1bfd-4171-8799-1a0b12b6786f"
      },
      "source": [
        "print(metric_df['f1-score'].describe())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "count    600.000000\n",
            "mean       0.562748\n",
            "std        0.099385\n",
            "min        0.103896\n",
            "25%        0.500000\n",
            "50%        0.557735\n",
            "75%        0.641580\n",
            "max        0.751773\n",
            "Name: f1-score, dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1tig0lcScDtV"
      },
      "source": [
        "На мой взгляд, для этой задачи recall является чуть важнее, чем precision. Важнее, чтобы реальному пользователю не достался нереватный документ, чем то, что какое-то количество релевантных документов отметят как нерелевантные. Поэтому F-меру можно немного улучшить. Взяв коэфицент beta = 1.5 в формуле F-меры, мы отдадим больший приоритет recall. Итоговая формула F-меры будет иметь вид:\n",
        "\n",
        "F-мера = 1.(4)*Precision×Recall/(Precision+Recall)\n",
        "\n",
        "Сделаем функцию по шаблону get_metriс для нахождения F-меры с beta=1.5 для каждого ассесора и посмотрим на статистики."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f4rs6kNoeFUh",
        "outputId": "090941b6-7080-4a24-a74e-9132ce88867d"
      },
      "source": [
        "group_login = df.groupby('uid', as_index=False)\n",
        "\n",
        "def get_fbeta(df):\n",
        "  result = fbeta_score(df['cjud'], df['jud'], beta=1.5)\n",
        "  return pd.Series(result, index=['fbeta-score'])\n",
        "\n",
        "fbeta_df = group_login.apply(get_fbeta)\n",
        "print(fbeta_df['fbeta-score'].describe())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "count    600.000000\n",
            "mean       0.639971\n",
            "std        0.094543\n",
            "min        0.104418\n",
            "25%        0.587668\n",
            "50%        0.645642\n",
            "75%        0.710569\n",
            "max        0.826334\n",
            "Name: fbeta-score, dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KUOlpvQegdhs"
      },
      "source": [
        "Также можно оценивать работу ассесора основываясь не только на его способностях, но и на сложности документа. Все документы оцениваются пять раз."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AjCf8m5bkr8B",
        "outputId": "99329ecb-2d5b-4072-9d0a-4a79ccb1c4bc"
      },
      "source": [
        "print(df['docid'].value_counts())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2047     5\n",
            "29860    5\n",
            "17698    5\n",
            "23841    5\n",
            "21792    5\n",
            "        ..\n",
            "12883    5\n",
            "14930    5\n",
            "8785     5\n",
            "10832    5\n",
            "0        5\n",
            "Name: docid, Length: 50000, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LVNguxSQnbIg"
      },
      "source": [
        "Расчитаем accuracy для каждого документа."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hQmrJ4rxlDAf",
        "outputId": "c9eb91ba-22a7-4866-996e-0a21a3db4596"
      },
      "source": [
        "group_docid = df.groupby('docid', as_index=False)\n",
        "\n",
        "def get_metriс(df):\n",
        "  result = dict()\n",
        "  result['accuracy'] = accuracy_score(df['cjud'], df['jud'])\n",
        "  return pd.Series(result, index=['accuracy'])\n",
        "\n",
        "doc_metric_df = group_docid.apply(get_metriс)\n",
        "print(doc_metric_df)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "       docid  accuracy\n",
            "0          0       1.0\n",
            "1          1       0.8\n",
            "2          2       0.8\n",
            "3          3       0.8\n",
            "4          4       0.8\n",
            "...      ...       ...\n",
            "49995  49995       1.0\n",
            "49996  49996       1.0\n",
            "49997  49997       0.8\n",
            "49998  49998       0.8\n",
            "49999  49999       1.0\n",
            "\n",
            "[50000 rows x 2 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YcYXMqtKm6-E"
      },
      "source": [
        "75% документов имееют accuracy больше 0.8. Это говорит о том, что действительно сложных документов для распознавания ассесорами не так много, чтобы оценивать их работу, принимая сложность документа во внимание."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CFfpo93ZmlUS",
        "outputId": "1f4c4d19-d7f7-4d66-c983-2d4ef07dda7b"
      },
      "source": [
        "print(doc_metric_df['accuracy'].describe())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "count    50000.000000\n",
            "mean         0.841288\n",
            "std          0.163129\n",
            "min          0.000000\n",
            "25%          0.800000\n",
            "50%          0.800000\n",
            "75%          1.000000\n",
            "max          1.000000\n",
            "Name: accuracy, dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MdbafZbFpzyV"
      },
      "source": [
        "Метрики классификации независимо от ассесора и документа, также могут использоваться для оценки работы ассесора путём сравнения их с его индивидуальными метриками"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z8gmT1r5pnUQ",
        "outputId": "bfb5e00c-1a4b-418c-baf3-6dc0ce280784"
      },
      "source": [
        "clf_report = classification_report(df['cjud'], df['jud'], output_dict=True)\n",
        "result = clf_report['1']\n",
        "result['accuracy'] = clf_report['accuracy']\n",
        "del result['support']\n",
        "total_metrics = pd.Series(result, index=['accuracy', 'precision', 'recall', 'f1-score'])\n",
        "print(total_metrics)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy     0.841288\n",
            "precision    0.418545\n",
            "recall       0.831087\n",
            "f1-score     0.556720\n",
            "dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ao2HhcXisLOn"
      },
      "source": [
        "Например, можно узнать долю людей с f1-score больше, чем в общем по dataframe"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ipZ5aVXdq_jS",
        "outputId": "8b9a61b6-b5e6-417f-dfea-888d7f293309"
      },
      "source": [
        "selected = metric_df.loc[metric_df['f1-score'] > total_metrics['f1-score']]\n",
        "print(len(selected) / len(metric_df))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.5066666666666667\n"
          ]
        }
      ]
    }
  ]
}